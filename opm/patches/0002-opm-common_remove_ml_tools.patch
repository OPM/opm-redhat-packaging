commit a9cd7aee10be093ed2c4cb4da6935aa6189f8791
Author: Arne Morten Kvarving <arne.morten.kvarving@sintef.no>
Date:   Mon Oct 13 13:07:52 2025 +0200

    remove ml_tools

diff --git a/python/opm/ml/ml_tools/README.md b/python/opm/ml/ml_tools/README.md
deleted file mode 100644
index 337de9272..000000000
--- a/python/opm/ml/ml_tools/README.md
+++ /dev/null
@@ -1,24 +0,0 @@
-These ML modules are extending the previous work done in Kerasify by Maevskikh and Rose licensed under the terms of the MIT
-license (https://github.com/moof2k/kerasify/tree/77a0c42). Kerasify is a library for running trained Keras models from a C++ platform.
-Our implementation is also compliant with the automatic differentiation approach used in OPM.
-The implementation works with Python v.3.9.0 and above (up to <=3.12.0).
-
--Unit tests are generated by:
-
-$ python3 generateunittests.py
-
-
--To compile run the unit tests:
-
-$ make ml_model_test
-$ ./bin/ml_model_test
-TEST tensor_test
-TEST dense_1x1
-TEST dense_10x1
-TEST dense_2x2
-TEST dense_10x10
-TEST dense_10x10x10
-TEST relu_10
-TEST dense_relu_10
-TEST dense_tanh_10
-TEST scalingdense_10x1
diff --git a/python/opm/ml/ml_tools/__init__.py b/python/opm/ml/ml_tools/__init__.py
deleted file mode 100644
index 27273543e..000000000
--- a/python/opm/ml/ml_tools/__init__.py
+++ /dev/null
@@ -1,2 +0,0 @@
-from .kerasify import *
-from .scaler_layers import *
diff --git a/python/opm/ml/ml_tools/kerasify.py b/python/opm/ml/ml_tools/kerasify.py
deleted file mode 100644
index 5ac321fc3..000000000
--- a/python/opm/ml/ml_tools/kerasify.py
+++ /dev/null
@@ -1,159 +0,0 @@
-
-#   Copyright (c) 2016 Robert W. Rose
-#   Copyright (c) 2018 Paul Maevskikh
-#   Copyright (c) 2024 NORCE
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-#
-# Note: This file is based on kerasify/kerasify.py
-
-import numpy as np
-import struct
-
-LAYER_SCALING = 1
-LAYER_UNSCALING = 2
-LAYER_DENSE = 3
-LAYER_ACTIVATION = 4
-
-ACTIVATION_LINEAR = 1
-ACTIVATION_RELU = 2
-ACTIVATION_SOFTPLUS = 3
-ACTIVATION_SIGMOID = 4
-ACTIVATION_TANH = 5
-ACTIVATION_HARD_SIGMOID = 6
-
-def write_scaling(f):
-    f.write(struct.pack('I', LAYER_SCALING))
-
-
-def write_unscaling(f):
-    f.write(struct.pack('I', LAYER_UNSCALING))
-
-
-def write_tensor(f, data, dims=1):
-    """
-    Writes tensor as flat array of floats to file in 1024 chunks,
-    prevents memory explosion writing very large arrays to disk
-    when calling struct.pack().
-    """
-    f.write(struct.pack('I', dims))
-
-    for stride in data.shape[:dims]:
-        f.write(struct.pack('I', stride))
-
-    data = data.ravel()
-    step = 1024
-    written = 0
-
-    for i in np.arange(0, len(data), step):
-        remaining = min(len(data) - i, step)
-        written += remaining
-        f.write(struct.pack(f'={remaining}f', *data[i: i + remaining]))
-
-    assert written == len(data)
-
-
-def write_floats(file, floats):
-    '''
-    Writes floats to file in 1024 chunks.. prevents memory explosion
-    writing very large arrays to disk when calling struct.pack().
-    '''
-    step = 1024
-    written = 0
-
-    for i in np.arange(0, len(floats), step):
-        remaining = min(len(floats) - i, step)
-        written += remaining
-        file.write(struct.pack('=%sf' % remaining, *floats[i:i+remaining]))
-
-    assert written == len(floats)
-
-def export_model(model, filename):
-    with open(filename, 'wb') as f:
-
-        def write_activation(activation):
-            if activation == 'linear':
-                f.write(struct.pack('I', ACTIVATION_LINEAR))
-            elif activation == 'relu':
-                f.write(struct.pack('I', ACTIVATION_RELU))
-            elif activation == 'softplus':
-                f.write(struct.pack('I', ACTIVATION_SOFTPLUS))
-            elif activation == 'tanh':
-                f.write(struct.pack('I', ACTIVATION_TANH))
-            elif activation == 'sigmoid':
-                f.write(struct.pack('I', ACTIVATION_SIGMOID))
-            elif activation == 'hard_sigmoid':
-                f.write(struct.pack('I', ACTIVATION_HARD_SIGMOID))
-            else:
-                assert False, f"Unsupported activation type:{activation}"
-
-        model_layers = [l for l in model.layers]
-
-        num_layers = len(model_layers)
-        f.write(struct.pack('I', num_layers))
-
-        for layer in model_layers:
-            layer_type = type(layer).__name__
-
-            if layer_type == 'MinMaxScalerLayer':
-                write_scaling(f)
-                feat_inf = layer.get_weights()[0]
-                feat_sup = layer.get_weights()[1]
-                f.write(struct.pack('f', layer.data_min))
-                f.write(struct.pack('f', layer.data_max))
-                f.write(struct.pack('f', feat_inf))
-                f.write(struct.pack('f', feat_sup))
-
-
-            elif layer_type == 'MinMaxUnScalerLayer':
-                write_unscaling(f)
-                feat_inf = layer.get_weights()[0]
-                feat_sup = layer.get_weights()[1]
-                f.write(struct.pack('f', layer.data_min))
-                f.write(struct.pack('f', layer.data_max))
-                f.write(struct.pack('f', feat_inf))
-                f.write(struct.pack('f', feat_sup))
-
-            elif layer_type == 'Dense':
-                weights = layer.get_weights()[0]
-                biases = layer.get_weights()[1]
-                activation = layer.get_config()['activation']
-
-                f.write(struct.pack('I', LAYER_DENSE))
-                f.write(struct.pack('I', weights.shape[0]))
-                f.write(struct.pack('I', weights.shape[1]))
-                f.write(struct.pack('I', biases.shape[0]))
-
-                weights = weights.flatten()
-                biases = biases.flatten()
-
-                write_floats(f, weights)
-                write_floats(f, biases)
-
-                write_activation(activation)
-
-
-            elif layer_type == 'Activation':
-                activation = layer.get_config()['activation']
-
-                f.write(struct.pack('I', LAYER_ACTIVATION))
-                write_activation(activation)
-
-            else:
-                assert False, f"Unsupported layer type:{layer_type}"
diff --git a/python/opm/ml/ml_tools/requirements.txt b/python/opm/ml/ml_tools/requirements.txt
deleted file mode 100644
index bf0e660ab..000000000
--- a/python/opm/ml/ml_tools/requirements.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-################################################################
-# Python v.3.9.0 and above
-python>=3.9.0<=3.12.0
-
-################################################################
-# Numpy 1.23.0 and above
-numpy>=1.23.0
-
-################################################################
-# TensorFlow v.2.1 and above include both CPU and GPU versions.
-tensorflow>=2.1.0
-
-################################################################
-# Keras v.2.12.0 and above
-keras>=2.12.0
-
-################################################################
diff --git a/python/opm/ml/ml_tools/scaler_layers.py b/python/opm/ml/ml_tools/scaler_layers.py
deleted file mode 100644
index e4f7864f7..000000000
--- a/python/opm/ml/ml_tools/scaler_layers.py
+++ /dev/null
@@ -1,201 +0,0 @@
-#   Copyright (c) 2024 NORCE
-#   Copyright (c) 2024 UiB
-#   This file is part of the Open Porous Media project (OPM).
-#   OPM is free software: you can redistribute it and/or modify
-#   it under the terms of the GNU General Public License as published by
-#   the Free Software Foundation, either version 3 of the License, or
-#   (at your option) any later version.
-#   OPM is distributed in the hope that it will be useful,
-#   but WITHOUT ANY WARRANTY; without even the implied warranty of
-#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-#   GNU General Public License for more details.
-#   You should have received a copy of the GNU General Public License
-#   along with OPM.  If not, see <http://www.gnu.org/licenses/>.
-
-"""Provide MinMax scaler layers for tensorflow.keras."""
-
-from __future__ import annotations
-
-from typing import Optional, Sequence
-
-import numpy as np
-import tensorflow as tf
-from numpy.typing import ArrayLike
-from tensorflow import keras
-from tensorflow.python.keras.engine.base_preprocessing_layer import (
-    PreprocessingLayer,
-)
-
-
-class ScalerLayer(keras.layers.Layer):
-    """MixIn to provide functionality for the Scaler Layer."""
-
-    data_min: tf.Tensor
-    data_max: tf.Tensor
-    min: tf.Tensor
-    scalar: tf.Tensor
-
-    def __init__(
-        self,
-        data_min: Optional[float | ArrayLike] = None,
-        data_max: Optional[float | ArrayLike] = None,
-        feature_range: Sequence[float] | np.ndarray | tf.Tensor = (0, 1),
-        **kwargs,
-    ) -> None:
-        super().__init__(**kwargs)
-        if feature_range[0] >= feature_range[1]:
-            raise ValueError("Feature range must be strictly increasing.")
-        self.feature_range: tf.Tensor = tf.convert_to_tensor(
-            feature_range, dtype=tf.float32
-        )
-        self._is_adapted: bool = False
-        if data_min is not None and data_max is not None:
-            self.data_min = tf.convert_to_tensor(data_min, dtype=tf.float32)
-            self.data_max = tf.convert_to_tensor(data_max, dtype=tf.float32)
-            self._adapt()
-
-    def build(self, input_shape: tuple[int, ...]) -> None:
-        """Initialize ``data_min`` and ``data_max`` with the default values if they have
-        not been initialized yet.
-
-        Args:
-            input_shape (tuple[int, ...]): _description_
-
-        """
-        if not self._is_adapted:
-            # ``data_min`` and ``data_max`` have the same shape as one input tensor.
-            self.data_min = tf.zeros(input_shape[1:])
-            self.data_max = tf.ones(input_shape[1:])
-            self._adapt()
-
-    def get_weights(self) -> list[ArrayLike]:
-        """Return parameters of the scaling.
-
-        Returns:
-            list[ArrayLike]: List with three elements in the following order:
-            ``self.data_min``, ``self.data_max``, ``self.feature_range``
-
-        """
-        return [self.feature_range[0], self.feature_range[1], self.data_min, self.data_max, self.feature_range]
-
-    def set_weights(self, weights: list[ArrayLike]) -> None:
-        """Set parameters of the scaling.
-
-        Args:
-            weights (list[ArrayLike]): List with three elements in the following order:
-            ``data_min``, ``data_max``, ``feature_range``
-
-        Raises:
-            ValueError: If ``feature_range[0] >= feature_range[1]``.
-
-        """
-        self.feature_range = tf.convert_to_tensor(weights[2], dtype=tf.float32)
-        if self.feature_range[0] >= self.feature_range[1]:
-            raise ValueError("Feature range must be strictly increasing.")
-        self.data_min = tf.convert_to_tensor(weights[0], dtype=tf.float32)
-        self.data_max = tf.convert_to_tensor(weights[1], dtype=tf.float32)
-
-    def adapt(self, data: ArrayLike) -> None:
-        """Fit the layer to the min and max of the data. This is done individually for
-        each input feature.
-
-        Note:
-            So far, this is only tested for 1 dimensional input and output. For higher
-            dimensional input and output some functionality might need to be added.
-
-        Args:
-            data: _description_
-
-        """
-        data = tf.convert_to_tensor(data, dtype=tf.float32)
-        self.data_min = tf.math.reduce_min(data, axis=0)
-        self.data_max = tf.math.reduce_max(data, axis=0)
-        self._adapt()
-
-    def _adapt(self) -> None:
-        if tf.math.reduce_any(self.data_min > self.data_max):
-            raise RuntimeError(
-                f"""self.data_min {self.data_min} cannot be larger than self.data_max
-                {self.data_max} for any element."""
-            )
-        self.scalar = tf.where(
-            self.data_max > self.data_min,
-            self.data_max - self.data_min,
-            tf.ones_like(self.data_min),
-        )
-        self.min = tf.where(
-            self.data_max > self.data_min,
-            self.data_min,
-            tf.zeros_like(self.data_min),
-        )
-        self._is_adapted = True
-
-
-class MinMaxScalerLayer(ScalerLayer, PreprocessingLayer):
-    """Scales the input according to MinMaxScaling.
-
-    See
-    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html
-    for an explanation of the transform.
-
-    """
-
-    def __init__(
-        self,
-        data_min: Optional[float | ArrayLike] = None,
-        data_max: Optional[float | ArrayLike] = None,
-        feature_range: Sequence[float] | np.ndarray | tf.Tensor = (0, 1),
-        **kwargs,
-    ) -> None:
-        super().__init__(data_min, data_max, feature_range, **kwargs)
-        self._name: str = "MinMaxScalerLayer"
-
-    def call(self, inputs: tf.Tensor) -> tf.Tensor:
-        if not self.is_adapted:
-            print(np.greater_equal(self.data_min, self.data_max))
-            raise RuntimeError(
-                """The layer has not been adapted correctly. Call ``adapt`` before using
-                the layer or set the ``data_min`` and ``data_max`` values manually.
-                """
-            )
-
-        # Ensure the dtype is correct.
-        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)
-        scaled_data = (inputs - self.min) / self.scalar
-        return (
-            scaled_data * (self.feature_range[1] - self.feature_range[0])
-        ) + self.feature_range[0]
-
-
-class MinMaxUnScalerLayer(ScalerLayer, tf.keras.layers.Layer):
-    """Unscales the input by applying the inverse transform of ``MinMaxScalerLayer``.
-
-    See
-    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html
-    for an explanation of the transformation.
-
-    """
-
-    def __init__(
-        self,
-        data_min: Optional[float | ArrayLike] = None,
-        data_max: Optional[float | ArrayLike] = None,
-        feature_range: Sequence[float] | np.ndarray | tf.Tensor = (0, 1),
-        **kwargs,
-    ) -> None:
-        super().__init__(data_min, data_max, feature_range, **kwargs)
-        self._name: str = "MinMaxUnScalerLayer"
-
-    def call(self, inputs: tf.Tensor) -> tf.Tensor:
-        if not self._is_adapted:
-            raise RuntimeError(
-                """The layer has not been adapted correctly. Call ``adapt`` before using
-                the layer or set the ``data_min`` and ``data_max`` values manually."""
-            )
-
-        # Ensure the dtype is correct.
-        inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)
-        unscaled_data = (inputs - self.feature_range[0]) / (
-            self.feature_range[1] - self.feature_range[0]
-        )
-        return unscaled_data * self.scalar + self.min
diff --git a/python/setup.py.in b/python/setup.py.in
index 17a680358..5a058343f 100644
--- a/python/setup.py.in
+++ b/python/setup.py.in
@@ -38,7 +38,6 @@ setup(
                 'opm.io.sim',
                 'opm.io.summary',
                 'opm.io.ecl',
-                'opm.ml.ml_tools',
                 'opm.tools',
                 'opm.util'
             ],
